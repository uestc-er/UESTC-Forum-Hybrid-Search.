#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import requests
import json
import time
import os
import re
from typing import List, Dict

class ForumCrawlerFinal:
    def __init__(self, forum_id: int, cookie: str, auth_token: str, max_pages: int = 10):
        self.forum_id = forum_id
        self.max_pages = max_pages
        
        # 1. åˆ—è¡¨ API (ç”¨äºè·å–å¸–å­æ¸…å•)
        self.list_api_url = "https://bbs.uestc.edu.cn/_/thread/list"
        
        # 2. è¯¦æƒ… API (æ ¹æ®ä½ æä¾›çš„å‡†ç¡® URL ä¿®æ”¹)
        self.detail_api_url = "https://bbs.uestc.edu.cn/_/post/list"
        
        self.session = requests.Session()
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
            'Referer': f'https://bbs.uestc.edu.cn/forum/{forum_id}',
            'Accept': 'application/json, text/plain, */*',
            'Cookie': cookie,
            'Authorization': auth_token 
        }
        self.session.headers.update(self.headers)

    def fetch_post_list(self, page: int) -> List[Dict]:
        """è·å–å¸–å­åˆ—è¡¨"""
        params = {
            'forum_id': self.forum_id,
            'page': page,
            'sort_by': 1,
            'forum_details': 1
        }
        try:
            resp = self.session.get(self.list_api_url, params=params, timeout=10)
            if resp.status_code == 401:
                print("âŒ åˆ—è¡¨ API 401 æœªæˆæƒï¼è¯·æ›´æ–° Tokenã€‚")
                return []
            resp.raise_for_status()
            data = resp.json()
            # å…¼å®¹ä¸åŒçš„è¿”å›ç»“æ„
            rows = data.get('data', {}).get('rows', [])
            if not rows:
                rows = data.get('rows', [])
            return rows
        except Exception as e:
            print(f"âŒ è·å–åˆ—è¡¨å¤±è´¥: {e}")
            return []

    def fetch_post_detail(self, thread_id: int) -> str:
        """
        ã€å…³é”®ä¿®æ”¹ã€‘ä½¿ç”¨ /_/post/list è·å–è¯¦æƒ…å…¨æ–‡
        """
        params = {
            'thread_id': thread_id,
            'page': 1,
            'thread_details': 1,
            'forum_details': 1
        }
        
        try:
            resp = self.session.get(self.detail_api_url, params=params, timeout=10)
            
            if resp.status_code != 200:
                print(f"    âš ï¸ è·å–è¯¦æƒ…å¤±è´¥ HTTP {resp.status_code}")
                return ""
            
            data = resp.json()
            
            # è§£æ rows
            # ç»“æ„å¯èƒ½æ˜¯ data['rows'] æˆ– data['data']['rows']ï¼Œæ ¹æ®ä½ æä¾›çš„ JSON æ˜¯ç›´æ¥åœ¨ data ä¸‹ï¼Ÿ
            # æˆ–è€…æ˜¯ data -> rowsã€‚é€šå¸¸ API è¿”å›æ˜¯ {"code":0, "data": { "rows": [...] } }
            # ä¸ºäº†ä¿é™©ï¼Œæˆ‘ä»¬åšåŒé‡æ£€æŸ¥
            
            rows = []
            if 'rows' in data: 
                rows = data['rows'] # é’ˆå¯¹ä½ æä¾›çš„ç‰‡æ®µç»“æ„
            elif 'data' in data and isinstance(data['data'], dict):
                rows = data['data'].get('rows', [])
            
            if not rows:
                return ""

            # å¯»æ‰¾æ¥¼ä¸» (is_first=1)
            target_message = ""
            for row in rows:
                if row.get('is_first') == 1:
                    target_message = row.get('message', '')
                    break
            
            # å¦‚æœæ²¡æ‰¾åˆ° is_firstï¼Œå°±å–ç¬¬ä¸€æ¡
            if not target_message and rows:
                target_message = rows[0].get('message', '')

            return self.clean_text(target_message)

        except Exception as e:
            print(f"    âš ï¸ è¯¦æƒ…è§£æå¼‚å¸¸: {e}")
            return ""

    def clean_text(self, raw_text):
        """æ¸…æ´—æ–‡æœ¬ï¼Œå¤„ç†ä½ æä¾›çš„ç¤ºä¾‹ä¸­çš„æ ¼å¼"""
        if not raw_text: return ""
        
        # 1. å¤„ç†å›¾ç‰‡/è¡¨æƒ…ä»£ç ï¼Œä¾‹å¦‚ ![1155](s)
        # æˆ‘ä»¬å¯ä»¥æŠŠå®ƒæ›¿æ¢ä¸ºç©ºï¼Œæˆ–è€…æ›¿æ¢ä¸º [è¡¨æƒ…]
        text = re.sub(r'!\[.*?\]\(.*?\)', '', raw_text)
        
        # 2. å»é™¤ HTML æ ‡ç­¾ (å¦‚æœæœ‰)
        text = re.sub(r'<[^>]+>', '', text)
        
        # 3. å¤„ç†è½¬ä¹‰å­—ç¬¦
        text = text.replace('\n', ' ').replace('\r', '')
        
        return text.strip()

    def crawl(self):
        all_data = []
        print(f"ğŸš€ å¼€å§‹å…¨é‡çˆ¬å– | Forum ID: {self.forum_id}")
        print("ğŸ’¡ æç¤ºï¼šçˆ¬å–å…¨æ–‡é€Ÿåº¦è¾ƒæ…¢ï¼Œè¯·è€å¿ƒç­‰å¾…...")

        for page in range(1, self.max_pages + 1):
            rows = self.fetch_post_list(page)
            if not rows:
                print("âš ï¸ æœ¬é¡µæ— æ•°æ®æˆ–å·²ç»“æŸã€‚")
                break
            
            print(f"âœ… ç¬¬ {page} é¡µ: å‘ç° {len(rows)} æ¡å¸–å­")
            
            for row in rows:
                thread_id = row.get('thread_id')
                title = row.get('subject')
                author = row.get('author')
                
                # 1. åˆ—è¡¨é¡µè‡ªå¸¦çš„æ‘˜è¦ (ä½œä¸ºå¤‡é€‰)
                summary = row.get('summary', '')
                
                # 2. è·å–å…¨æ–‡
                # ç¨å¾®å»¶æ—¶ï¼Œé¿å…å¹¶å‘è¿‡é«˜è¢«å°
                time.sleep(0.5) 
                full_content = self.fetch_post_detail(thread_id)
                
                # å¦‚æœè¯¦æƒ…é¡µæ²¡å–åˆ°ï¼Œå°±ç”¨æ‘˜è¦é¡¶æ›¿
                final_content = full_content if len(full_content) > len(summary) else summary

                # æ—¶é—´å¤„ç†
                try:
                    ts = time.strftime('%Y-%m-%d %H:%M', time.localtime(row.get('dateline', 0)))
                except:
                    ts = "æœªçŸ¥æ—¶é—´"

                item = {
                    "id": str(thread_id),
                    "title": title,
                    "author": author,
                    "timestamp": ts,
                    "url": f"https://bbs.uestc.edu.cn/forum.php?mod=viewthread&tid={thread_id}",
                    "content": final_content
                }
                all_data.append(item)
                print(f"  -> {title[:15]}... (æ­£æ–‡:{len(final_content)}å­—)")
            
            self.save_data(all_data)
            
        print(f"\nğŸ‰ çˆ¬å–ç»“æŸï¼å…±æ”¶é›† {len(all_data)} æ¡æ•°æ®ã€‚")

    def save_data(self, data):
        output_file = "data/posts_data.json"
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

def main():
    # ================= é…ç½®åŒº =================
    FORUM_ID = 370
    
    # âš ï¸ è¯·åŠ¡å¿…æ›´æ–° Cookie å’Œ Tokenï¼Œå› ä¸ºå®ƒä»¬æœ‰æ•ˆæœŸå¾ˆçŸ­
    COOKIE = ""
    AUTH_TOKEN = ""
    # =========================================
    
    if "ä½ çš„" in AUTH_TOKEN:
        print("âŒ è¯·å¡«å…¥ Cookie å’Œ Token")
        return

    crawler = ForumCrawlerFinal(FORUM_ID, COOKIE, AUTH_TOKEN, max_pages=5)
    crawler.crawl()

if __name__ == "__main__":
    main()