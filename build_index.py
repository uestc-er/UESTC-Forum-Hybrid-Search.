#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç´¢å¼•æ„å»ºæ¨¡å—
åŠŸèƒ½ï¼šè¯»å–JSONæ•°æ®ï¼Œåˆ†åˆ«æ„å»º"å‘é‡ç´¢å¼•"å’Œ"å…³é”®è¯ç´¢å¼•"
"""

import json
import pickle
import os
import sys
import math
from typing import List, Dict, Any
from tqdm import tqdm # å¯¼å…¥è¿›åº¦æ¡åº“

# å¯¼å…¥å¿…è¦çš„åº“
try:
    import jieba
    from rank_bm25 import BM25Okapi
    from sentence_transformers import SentenceTransformer
    import chromadb
    from chromadb.config import Settings
except ImportError as e:
    print(f"å¯¼å…¥åº“å¤±è´¥: {e}")
    print("è¯·å…ˆå®‰è£…ä¾èµ–: pip install chromadb sentence-transformers rank_bm25 jieba tqdm")
    sys.exit(1)


class IndexBuilder:
    # ã€ä¿®æ”¹ç‚¹1ã€‘é»˜è®¤è·¯å¾„æ”¹ä¸º cleaned ç‰ˆæœ¬
    def __init__(self, data_path: str = "data/posts_data_cleaned.json", 
                 chroma_db_path: str = "chroma_db",
                 embedding_model_name: str = "shibing624/text2vec-base-chinese"):
        self.data_path = data_path
        self.chroma_db_path = chroma_db_path
        self.embedding_model_name = embedding_model_name
        
        os.makedirs(os.path.dirname(data_path), exist_ok=True)
        os.makedirs(chroma_db_path, exist_ok=True)
        
        self.embedding_model = None
        self.chroma_client = None
        self.collection = None
        
    def load_data(self) -> List[Dict[str, Any]]:
        """åŠ è½½JSONæ•°æ®"""
        if not os.path.exists(self.data_path):
            print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.data_path}")
            print("è¯·å…ˆè¿è¡Œ clean_data.py è¿›è¡Œæ•°æ®æ¸…æ´—ï¼")
            return []
        
        try:
            with open(self.data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"ğŸ“š æˆåŠŸåŠ è½½ {len(data)} æ¡å¸–å­æ•°æ®")
            return data
        except Exception as e:
            print(f"åŠ è½½æ•°æ®å¤±è´¥: {e}")
            return []
    
    def initialize_models(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹å’ŒChromaDBå®¢æˆ·ç«¯"""
        print("â³ æ­£åœ¨åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ (é¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½ï¼Œçº¦400MB)...")
        try:
            self.embedding_model = SentenceTransformer(
                self.embedding_model_name,
                device='cpu' # ç¬”è®°æœ¬ä½¿ç”¨ CPU å³å¯
            )
            print(f"âœ… åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ: {self.embedding_model_name}")
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ä¸­æ–‡æ¨¡å‹å¤±è´¥: {e}")
            print("ğŸ”„ å°è¯•ä½¿ç”¨å¤‡ç”¨æ¨¡å‹...")
            self.embedding_model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2", device='cpu')
        
        print("â³ æ­£åœ¨åˆå§‹åŒ– ChromaDB...")
        try:
            self.chroma_client = chromadb.PersistentClient(path=self.chroma_db_path)
            print("âœ… ChromaDB å®¢æˆ·ç«¯å°±ç»ª")
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ– ChromaDB å¤±è´¥: {e}")
            raise
    
    def build_vector_index(self, posts: List[Dict[str, Any]]) -> bool:
        """æ„å»ºå‘é‡ç´¢å¼• (ChromaDB) - æ”¯æŒåˆ†æ‰¹å¤„ç†"""
        if not posts: return False
        
        try:
            # é‡ç½® Collection
            try:
                self.chroma_client.delete_collection("forum_posts")
            except:
                pass
            
            self.collection = self.chroma_client.create_collection(
                name="forum_posts",
                metadata={"description": "UESTC Forum Posts"}
            )
            
            # å‡†å¤‡æ•°æ®
            print("ğŸ”„ æ­£åœ¨å‡†å¤‡å‘é‡æ•°æ®...")
            ids = []
            documents = []
            metadatas = []
            
            for i, post in enumerate(posts):
                # ç¡®ä¿ ID æ˜¯å­—ç¬¦ä¸²
                post_id = str(post.get('id', i)) 
                
                # ç»„åˆæ ‡é¢˜å’Œå†…å®¹ï¼Œè®©è¯­ä¹‰æ›´ä¸°å¯Œ
                title = post.get('title', 'æ— æ ‡é¢˜')
                content = post.get('content', '')
                # å¦‚æœæ­£æ–‡å¤ªçŸ­ï¼Œé‡å¤ä¸€ä¸‹æ ‡é¢˜å¢å¼ºæƒé‡
                doc_text = f"{title}\n{content}" if len(content) > 5 else f"{title}\n{title}"
                
                # é•¿åº¦æˆªæ–­ (Chroma é™åˆ¶)
                if len(doc_text) > 8000: doc_text = doc_text[:8000]
                
                ids.append(post_id)
                documents.append(doc_text)
                metadatas.append({
                    'title': title,
                    'author': post.get('author', 'æœªçŸ¥'),
                    'url': post.get('url', ''),
                    'timestamp': str(post.get('timestamp', '')),
                    'id': post_id
                })
            
            # ã€ä¿®æ”¹ç‚¹2ã€‘åˆ†æ‰¹å†™å…¥ (Batch Processing)
            BATCH_SIZE = 64
            total_batches = math.ceil(len(ids) / BATCH_SIZE)
            
            print(f"ğŸš€ å¼€å§‹å‘é‡åŒ–å¹¶å­˜å…¥æ•°æ®åº“ (å…± {len(ids)} æ¡ï¼Œåˆ† {total_batches} æ‰¹)...")
            
            for i in tqdm(range(0, len(ids), BATCH_SIZE), desc="å‘é‡åŒ–è¿›åº¦"):
                end = i + BATCH_SIZE
                batch_ids = ids[i:end]
                batch_docs = documents[i:end]
                batch_metas = metadatas[i:end]
                
                # ç”Ÿæˆå‘é‡
                batch_embeddings = self.embedding_model.encode(
                    batch_docs, 
                    normalize_embeddings=True # å½’ä¸€åŒ–å‘é‡ï¼Œè¿™å¯¹ä½™å¼¦ç›¸ä¼¼åº¦å¾ˆé‡è¦
                ).tolist()
                
                # å†™å…¥ Chroma
                self.collection.add(
                    ids=batch_ids,
                    embeddings=batch_embeddings,
                    documents=batch_docs,
                    metadatas=batch_metas
                )
            
            print(f"âœ… å‘é‡ç´¢å¼•æ„å»ºå®Œæˆï¼")
            return True
            
        except Exception as e:
            print(f"âŒ æ„å»ºå‘é‡ç´¢å¼•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def build_keyword_index(self, posts: List[Dict[str, Any]]) -> bool:
        """æ„å»ºå…³é”®è¯ç´¢å¼• (BM25)"""
        if not posts: return False
        
        try:
            print("ğŸ—ï¸ æ­£åœ¨æ„å»ºå…³é”®è¯ç´¢å¼• (BM25)...")
            
            tokenized_corpus = []
            doc_mapping = [] 
            
            # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
            for post in tqdm(posts, desc="åˆ†è¯è¿›åº¦"):
                # ç»„åˆæ ‡é¢˜å’Œå†…å®¹
                text = f"{post.get('title', '')} {post.get('content', '')}"
                
                # jieba åˆ†è¯
                tokens = jieba.lcut_for_search(text)
                
                # ç®€å•çš„åœç”¨è¯è¿‡æ»¤ (è¿‡æ»¤æ‰æ ‡ç‚¹å’Œå•å­—)
                filtered_tokens = [t for t in tokens if len(t.strip()) > 1]
                
                tokenized_corpus.append(filtered_tokens)
                doc_mapping.append(post) # å­˜ä¸‹åŸå§‹æ•°æ®ï¼Œæ–¹ä¾¿æ£€ç´¢æ—¶æŸ¥é˜…
            
            # æ„å»ºæ¨¡å‹
            bm25 = BM25Okapi(tokenized_corpus)
            
            # ä¿å­˜
            index_data = {
                'bm25_model': bm25,
                'doc_mapping': doc_mapping
            }
            
            output_path = "data/bm25_index.pkl"
            with open(output_path, 'wb') as f:
                pickle.dump(index_data, f)
            
            print(f"âœ… å…³é”®è¯ç´¢å¼•å·²ä¿å­˜: {output_path}")
            return True
            
        except Exception as e:
            print(f"âŒ æ„å»ºå…³é”®è¯ç´¢å¼•å¤±è´¥: {e}")
            return False
    
    def build_all_indices(self):
        print("ğŸš€ å¯åŠ¨ç´¢å¼•æ„å»ºæµç¨‹...")
        posts = self.load_data()
        if not posts: return
        
        self.initialize_models()
        
        v_ok = self.build_vector_index(posts)
        k_ok = self.build_keyword_index(posts)
        
        if v_ok and k_ok:
            print("\nğŸ‰ğŸ‰ğŸ‰ æ‰€æœ‰ç´¢å¼•æ„å»ºæˆåŠŸï¼")
            print(f"ğŸ“‚ å‘é‡åº“å­˜æ”¾äº: {self.chroma_db_path}")
            print(f"ğŸ“‚ BM25 å­˜æ”¾äº: data/bm25_index.pkl")
        else:
            print("\nâš ï¸ å³ä½¿éƒ¨åˆ†å¤±è´¥ï¼Œæ‚¨å¯èƒ½ä»å¯è¿è¡Œæœç´¢ï¼Œä½†åŠŸèƒ½å—é™ã€‚")

if __name__ == "__main__":
    # ç¡®ä¿ data ç›®å½•å­˜åœ¨
    if not os.path.exists("data"):
        os.makedirs("data")
        
    builder = IndexBuilder()
    builder.build_all_indices()